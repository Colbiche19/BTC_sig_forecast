{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read me"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This package can be used for the forecast of any time series using three prediction paradigms. \n",
    "\n",
    "The user has to fill the inputs section. The parameters of the signature can be chosen. We however provide default values that we usually find the most appropriate. \n",
    "\n",
    "A time series has to be loaded. The logarithmic return time series is built upon the price column of this dataset. Please fill the column names appropriately. \n",
    "\n",
    "Some paradigm parameters can be changed in the first code box in every paradigm section. Please feel free to play with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "#Length of the interval on which the signature is computed\n",
    "length_a=3\n",
    "#Depth of the signature (order between 2 and 4)\n",
    "sig_order=3\n",
    "#Load a file with a time column and a Price column\n",
    "data_csv=pd.read_csv(r\"path//\")\n",
    "dataset=data_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build returns \n",
    "dataset['Log_return']=np.log(dataset['Price'])-np.log(BTC_price_time['Price'].shift(+1))\n",
    "dataset['Sign_return']=np.where(dataset.Log_return>=0,1,0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please run the code here below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import esig.tosig as ts\n",
    "import sklearn\n",
    "import xgboost\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetWindow(x,h_window =30,f_window=10):\n",
    "\n",
    "    # First window\n",
    "    X = np.array(x.iloc[:h_window,]).reshape(1,-1)\n",
    "   \n",
    "    # Append next window\n",
    "    for i in range(1,len(x)-h_window+1):\n",
    "        x_i = np.array(x.iloc[i:i+h_window,]).reshape(1,-1)\n",
    "        X = np.append(X,x_i, axis=0)\n",
    "        \n",
    "    # Cut the end that we can't use to predict future price\n",
    "    rolling_window = (pd.DataFrame(X)).iloc[:-f_window,]\n",
    "    return rolling_window\n",
    "\n",
    "#input = panda, historical window, future window\n",
    "def GetNextMean(x,h_window=30,f_window=10):\n",
    "    return pd.DataFrame((x.rolling(f_window).mean().iloc[h_window+f_window-1:,]))\n",
    "\n",
    "#Function add time to the data set\n",
    "def AddTime(X):\n",
    "    t = np.linspace(0,1,len(X))\n",
    "    return np.c_[t, X]\n",
    "\n",
    "\n",
    "#Function for Lead lag transform\n",
    "def Lead(X):\n",
    "    \n",
    "    s = X.shape\n",
    "    x_0 = X[:,0]\n",
    "    Lead = np.delete(np.repeat(x_0,2),0).reshape(-1,1)\n",
    "     \n",
    "    for j in range(1,s[1]):\n",
    "        x_j = X[:,j]\n",
    "        x_j_lead = np.delete(np.repeat(x_j,2),0).reshape(-1,1)\n",
    "        Lead = np.concatenate((Lead,x_j_lead), axis =1)\n",
    "     \n",
    "    return Lead\n",
    "\n",
    "#Function for Lead lag transform\n",
    "def Lag(X):\n",
    "    \n",
    "    s = X.shape\n",
    "    x_0 = X[:,0]\n",
    "    Lag = np.delete(np.repeat(x_0,2),-1).reshape(-1,1)\n",
    "  \n",
    "    for j in range(1,s[1]):\n",
    "        x_j = X[:,j]\n",
    "        x_j_lag  = np.delete(np.repeat(x_j,2),-1).reshape(-1,1)\n",
    "        Lag = np.concatenate((Lag,x_j_lag), axis = 1)\n",
    "        \n",
    "    return Lag\n",
    "        \n",
    "\n",
    "# We use only open price\n",
    "close_price = dataset.loc[:,'Log_return']\n",
    "h_window = length_a\n",
    "f_window = 1\n",
    "sig_level = sig_order\n",
    "\n",
    "# mean next price\n",
    "y = GetNextMean(close_price, h_window = h_window , f_window = f_window)\n",
    "\n",
    "#GetNextMean(close_price, h_window = h_window , f_window = f_window)\n",
    "\n",
    "# normal window features\n",
    "X_window = AddTime(GetWindow(close_price, h_window = h_window, f_window = f_window))\n",
    "X_window = pd.DataFrame(X_window)\n",
    "\n",
    "\n",
    "# signature features\n",
    "#Consider only area that has at least f_window future prices left\n",
    "close_price_slice = close_price.iloc[0:(len(close_price)-(f_window))]\n",
    "close_price_array = np.array(close_price_slice).reshape(-1,1)\n",
    "lag = Lag(close_price_array)\n",
    "lead = Lead(AddTime(close_price_array))\n",
    "#concatenate everything to get a datastream\n",
    "stream = np.concatenate((lead,lag), axis = 1)\n",
    "X_sig = [ts.stream2sig(stream[0:2*h_window-1], sig_level)]\n",
    "print(stream[11])\n",
    "for i in range(1,(len(close_price)-(f_window)-(h_window)+1)):\n",
    "    stream_i = stream[2*i: 2*(i+h_window)-1]\n",
    "    signature_i = [ts.stream2sig(stream_i, sig_level)]\n",
    "    X_sig = np.append(X_sig, signature_i, axis=0)\n",
    "\n",
    "X_sig = pd.DataFrame(X_sig)\n",
    "\n",
    "from sklearn import decomposition as decomp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "scaler=StandardScaler()#instantiate\n",
    "scaler.fit(X_sig) # compute the mean and standard which will be used in the next command\n",
    "X_scaled=scaler.transform(X_sig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paradigm 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=BTC_price_time.loc[:,'Sign_return']\n",
    "#provide the wished length of the test set\n",
    "test_len = 2000\n",
    "train_len = len(y) - test_len\n",
    "X_train = X_scaled[:train_len-h_window,]\n",
    "y_train = y.iloc[h_window:train_len,]\n",
    "X_test = X_scaled[train_len-h_window:,]\n",
    "y_test = y.iloc[train_len:len(y),]\n",
    "X_train = pd.DataFrame(X_train)\n",
    "y_train = pd.DataFrame(y_train)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "y_test = pd.DataFrame(y_test)\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "y_train= y_train.fillna(y_train.mean())\n",
    "X_test = X_test.fillna(X_train.mean())\n",
    "y_test= y_test.fillna(y_train.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results with Elastic Net\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# sklearn.linear_model.SGDClassifier(loss='log', penalty='elasticnet', alpha=0.0001, l1_ratio=0.15)\n",
    "param_search = {'alpha': [1e-5,1e-4,1e-3, 1e-2,1e-1,2e-1, 5e-1, 1], 'l1_ratio':[0.15,0.3,0.45,0.6,0.75,0.9]}\n",
    "myModel = GridSearchCV(estimator=SGDClassifier(alpha = 0.1,\n",
    "                                       \n",
    "                                    \n",
    "                                       max_iter = 10e5,\n",
    "                                       penalty='elasticnet',\n",
    "                                       loss='log'),\n",
    "                        param_grid = param_search,\n",
    "                        cv = TimeSeriesSplit(n_splits=8),\n",
    "                        n_jobs=-1)\n",
    "\n",
    "myModel.fit(X_train, y_train)\n",
    "\n",
    "#Make predictions\n",
    "y_train_predict = myModel.predict(X_train)\n",
    "y_test_predict = myModel.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_test_predict)\n",
    "print(\"confusion matrix with an multiple logistic regression model with Elastic Net regularisation\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results with Random Forest\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#mean absolute percentage error\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "#Perform a GridsearchCV\n",
    "param_grid = [\n",
    "    {'classifier' : [LogisticRegression()],\n",
    "     'classifier__penalty' : [ 'l2'],\n",
    "    'classifier__C' : np.logspace(-20, -4, 4, 20, 50),\n",
    "    'classifier__solver' : ['newton-cg']},\n",
    "    {'classifier' : [RandomForestClassifier()],\n",
    "    'classifier__n_estimators' : list(range(10,150,160)),\n",
    "    'classifier__max_features' : list(range(5,20, 40))}\n",
    "]\n",
    "pipe = Pipeline([('classifier' , RandomForestClassifier())])\n",
    "myModel = GridSearchCV(pipe, param_grid = param_grid, cv = 8, verbose=True, n_jobs=-3)\n",
    "\n",
    "# myModel.fit(X_scaled[:len(X_scaled)-240-1], values[h_window+1:aa-240])\n",
    "myModel.fit(X_train, y_train)\n",
    "#Make predictions\n",
    "# y_train_predict = myModel.predict(X_scaled)\n",
    "y_test_predict = myModel.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_test_predict)\n",
    "print(\"confusion matrix with a RF model\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results with XGBoost\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics   #Additional scklearn functions\n",
    "from sklearn.model_selection import GridSearchCV  #Perforing grid search\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#GridsearchCV    \n",
    "param_search = {\n",
    "    'max_depth':[4,5,6]\n",
    "    ,'min_child_weight':[4,5,6]\n",
    "    ,'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "\n",
    "myModel = GridSearchCV(estimator=XGBRegressor(\n",
    "                        objective='binary:logistic',\n",
    "                        learning_rate=0.01,\n",
    "                        n_estimators=500,\n",
    "                        max_depth=5,\n",
    "                        min_child_weight=5,\n",
    "                        gamma=0,\n",
    "                        subsample=0.8,\n",
    "                        colsample_bytree=0.8, \n",
    "                        eval_metric ='mae',\n",
    "                        reg_alpha=0.05\n",
    "                        ),\n",
    "                       param_grid = param_search,\n",
    "                       cv = TimeSeriesSplit(n_splits=4),n_jobs=-1\n",
    "                      )\n",
    "\n",
    "#Fit model\n",
    "myModel.fit(X_train, y_train)\n",
    "\n",
    "#Make predictions\n",
    "# y_train_predict = myModel.predict(X_train)\n",
    "y_test_predict = myModel.predict(X_test)\n",
    "\n",
    "\n",
    "positif=0\n",
    "negatif=0\n",
    "extreme=0\n",
    "inter=0\n",
    "i=0\n",
    "hg,hd,bg,bd=0\n",
    "while i < len(y_test_predict):\n",
    "    if y_test_predict[i]>0.5:\n",
    "        if np.asscalar(y_test.iloc[i])>0.5:\n",
    "            hg+=1\n",
    "            extreme+=1\n",
    "        else:\n",
    "            hd+=1\n",
    "            extreme+=1\n",
    "    elif y_test_predict[i]<0.5:\n",
    "        if np.asscalar(y_test.iloc[i])<0.5:\n",
    "            bd+=1\n",
    "        else:\n",
    "            bg+=1\n",
    "    else:\n",
    "        inter+=1\n",
    "    i+=1\n",
    "    \n",
    "print(\"confusion matrix with an XGB model\")\n",
    "print('[',hg,hd)\n",
    "print(' ', bg,bd,']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paradigm 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forecast horizon\n",
    "h=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "#Forecast signature terms with ARIMA\n",
    "o=2\n",
    "list_col=[]\n",
    "X_scaled=pd.DataFrame(X_scaled)\n",
    "X_scaled=X_scaled.fillna(X_scaled.mean())\n",
    "while o<3:\n",
    "    series=X_scaled.iloc[len(X_scaled)-1000:,o]\n",
    "    print(len(series))\n",
    "    p=0\n",
    "    list_rows=[]\n",
    "    param=h\n",
    "    while p<param:\n",
    "        train = series[:len(series)-param+p]\n",
    "        test = series[len(series)-param+p:len(series)-param+p+1]\n",
    "        if p==0:\n",
    "            print('test',test)\n",
    "        model= auto_arima(series, start_p=0, start_q=0,\n",
    "                               max_p=3, max_q=3, \n",
    "                               seasonal=False,\n",
    "                               d=1, trace=False,\n",
    "                               error_action='ignore',  \n",
    "                               suppress_warnings=True, \n",
    "                               stepwise=True)\n",
    "\n",
    "        \n",
    "        model.fit(train)\n",
    "        future_forecast = model.predict(n_periods=len(test))\n",
    "        list_rows.append(future_forecast)\n",
    "        \n",
    "        p+=1\n",
    "    list_col.append(list_rows)\n",
    "    print(list_col)\n",
    "    \n",
    "    o+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_returns=dataset['Log_return'][len(dataset['Log_return'])-h:len(dataset['Log_return'])]\n",
    "to_add=dataset['Log_return'][len(dataset['Log_return'])-h-length_a:len(dataset['Log_return'])-length_a]\n",
    "list_returns_for=[]\n",
    "k=0\n",
    "for i in list_col:\n",
    "    list_returns_for.append(i+np.asarray(to_add[k:k+1,]))\n",
    "    k+=1\n",
    "    \n",
    "good=0\n",
    "bad=0\n",
    "m=0\n",
    "while m<h:\n",
    "    if (i[m].item()>0 and true_returns[m:m+1,].item()>0)or (i[m].item()<0 and true_returns[m:m+1,].item()<0):\n",
    "        good+=1\n",
    "    m+=1\n",
    "print(\"accuracy of the paradigm 2\")\n",
    "print(good/h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paradigm 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "y_vector=y.iloc[h_window:len(y)]\n",
    "y_train=y_vector[9:]\n",
    "list_x_train=[]\n",
    "k=9\n",
    "while k<len(X_scaled):\n",
    "    list_temp=[]\n",
    "    p=0\n",
    "    while p<=9:\n",
    "        list_temp.append(X_scaled[k-p])\n",
    "        p+=1\n",
    "    list_x_train.append(list_temp)\n",
    "    k+=1\n",
    "\n",
    "list_fin_train=[]\n",
    "for i in list_x_train:\n",
    "    list_temp=[]\n",
    "    for j in i:\n",
    "        for k in j:\n",
    "            list_temp.append(k)\n",
    "    list_fin_train.append(list_temp)\n",
    "\n",
    "X_full=np.asarray(list_fin_train)\n",
    "y_full=np.asarray(y_train).reshape((len(y_train), 1))\n",
    "\n",
    "X_train=X_full[1:len(X_full)-1999]\n",
    "X_test=X_full[len(X_full)-1999:]\n",
    "\n",
    "y_train=y_full[1:len(y_full)-1999]\n",
    "y_test=y_full[len(y_full)-1999:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1+ np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1.0 - x)\n",
    "def ReLU(x): \n",
    "    j=0\n",
    "    for i in x:\n",
    "        p=0\n",
    "        for k in i:\n",
    "            if k <0:\n",
    "                x[j][p]=0\n",
    "            p+=1\n",
    "        j+=1\n",
    "    return x\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y, lamb1, learning_r):\n",
    "        self.input = x\n",
    "        list_weights1 = []\n",
    "        j=0\n",
    "        while j<self.input.shape[1]:\n",
    "            list_temp=[]\n",
    "            k=j//40\n",
    "            m=0\n",
    "            list_temp+=[0]*180\n",
    "            r=18*(k+1)\n",
    "            b=r-18\n",
    "            for i in range(b,r):\n",
    "                list_temp[i]=random.uniform(-150,150)\n",
    "            list_weights1.append(np.asarray(list_temp))\n",
    "            j+=1\n",
    "            \n",
    "        self.weights1   = np.asarray(list_weights1)\n",
    "        \n",
    "        list_weights2=[]\n",
    "        j=0\n",
    "        while j<180:\n",
    "            list_temp=[]\n",
    "            k=j//18\n",
    "            m=0\n",
    "            list_temp+=[0]*60\n",
    "            r=6*(k+1)\n",
    "            b=r-6\n",
    "            for i in range(b,r):\n",
    "                list_temp[i]=random.uniform(-150,150)\n",
    "            list_weights2.append(np.asarray(list_temp))\n",
    "            j+=1\n",
    "        \n",
    "        self.weights2   = np.asarray(list_weights2)\n",
    "        \n",
    "        list_weights3=[]\n",
    "        j=0\n",
    "        while j<60:\n",
    "            list_temp=[]\n",
    "            k=j//6\n",
    "            m=0\n",
    "            list_temp+=[0]*10\n",
    "            r=1*(k+1)\n",
    "            b=r-1\n",
    "            for i in range(b,r):\n",
    "                list_temp[i]=random.uniform(-15,15)\n",
    "            list_weights3.append(np.asarray(list_temp))\n",
    "            j+=1\n",
    "        self.weights3   = np.asarray(list_weights3)\n",
    "        self.weights4   = np.random.uniform(low=-15,high=15, size=(10,1))\n",
    "        self.weights5   = np.random.uniform(low=-15,high=15, size=(1,1))\n",
    "        self.y          = y\n",
    "        self.output     = np.zeros(self.y.shape)\n",
    "        \n",
    "        self.lamb1=lamb1\n",
    "        self.learning_r=learning_r\n",
    "        \n",
    "    def feedforward(self):\n",
    "        #ON peut tenter avec ReLU aussi \n",
    "#         self.layer1=ReLU(np.dot(self.input, self.weights1))\n",
    "#         self.layer2=ReLU(np.dot(self.layer1, self.weights2))\n",
    "#         self.layer3=ReLU(np.dot(self.layer2, self.weights3))\n",
    "#         self.layer4=ReLU(np.dot(self.layer3, self.weights4))\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.layer2 = sigmoid(np.dot(self.layer1, self.weights2))\n",
    "        self.layer3 = sigmoid(np.dot(self.layer2, self.weights3))\n",
    "        self.layer4 = sigmoid(np.dot(self.layer3, self.weights4))\n",
    "        self.output = sigmoid(np.dot(self.layer4, self.weights5))\n",
    "        \n",
    "    def backprop(self):\n",
    "        # application of the chain rule to find derivative of the loss function with respect to weights\n",
    "        self.list3=[]\n",
    "        self.list2=[]\n",
    "        self.list1=[]\n",
    "        self.list_input=[]\n",
    "        self.list_weights3=[]\n",
    "        self.list_weights2=[]\n",
    "        p=0\n",
    "        while p<10:\n",
    "            list3=[]\n",
    "            \n",
    "            for i in self.layer3:\n",
    "                list3.append(i[p])\n",
    "            list2=[]\n",
    "            for i in self.layer2:\n",
    "                list2.append(i[p*6:(p+1)*6])\n",
    "            list1=[]\n",
    "            for i in self.layer1:\n",
    "                list1.append(i[p*18:(p+1)*18])\n",
    "            list_input=[]\n",
    "            for i in self.input:\n",
    "                list_input.append(i[p*40:(p+1)*40])\n",
    "            list_weights3=[]\n",
    "            \n",
    "            f=0\n",
    "            for i in self.weights3:\n",
    "                if f>=(p)*6 and f<(p+1)*6:\n",
    "                    list_weights3.append(i[p])\n",
    "                f+=1\n",
    "\n",
    "            list_weights2=[]\n",
    "\n",
    "            for i in self.weights2[p*18:(p+1)*18]:\n",
    "                list_weights2.append(i[p*6:(p+1)*6])\n",
    "\n",
    "            \n",
    "            self.list_input.append(np.asarray(list_input))\n",
    "            self.list1.append(np.asarray(list1))\n",
    "            self.list2.append(np.asarray(list2))\n",
    "            self.list3.append(np.asarray(list3))\n",
    "            self.list_weights3.append(np.asarray(list_weights3))\n",
    "            self.list_weights2.append(np.asarray(list_weights2))\n",
    "            \n",
    "            p+=1\n",
    "        \n",
    "        self.list_input=np.asarray(self.list_input)     \n",
    "        self.list1=np.asarray(self.list1)\n",
    "        self.list2=np.asarray(self.list2)\n",
    "        self.list3=np.asarray(self.list3)\n",
    "        self.list3=self.list3.reshape(10,7988,1)\n",
    "        self.list_weights3=np.asarray(self.list_weights3)\n",
    "        self.list_weights2=np.asarray(self.list_weights2)\n",
    "        \n",
    "        lambda_1=self.lamb1\n",
    "        lambda_2=0.5\n",
    "        learning_rate=self.learning_r\n",
    "        error_output=(self.y*np.log(self.output)+(1-self.y)*np.log(1-self.output))\n",
    "        d_error_output=self.y/self.output +(1-self.y)/(self.output-1)\n",
    "\n",
    "        d_weights5 = np.dot(self.layer4.T, (-2*(self.y-self.output) * sigmoid_derivative(self.output)))\n",
    "        d_weights4 = np.dot(self.layer3.T, (np.dot((-2*(self.y-self.output) * sigmoid_derivative(self.output)), self.weights5.T) * sigmoid_derivative(self.layer4)))\n",
    "        error_layer3=(np.dot((np.dot(-2*(self.y-self.output) * sigmoid_derivative(self.output), self.weights5.T) * sigmoid_derivative(self.layer4)),self.weights4.T))\n",
    "        error_layer2=np.dot((np.dot((np.dot(-2*(self.y-self.output) * sigmoid_derivative(self.output), self.weights5.T) * sigmoid_derivative(self.layer4)),self.weights4.T) * sigmoid_derivative(self.layer3)),self.weights3.T)\n",
    "        error_layer1=np.dot((np.dot((np.dot((np.dot(-2*(self.y-self.output) * sigmoid_derivative(self.output), self.weights5.T) * sigmoid_derivative(self.layer4)),self.weights4.T) * sigmoid_derivative(self.layer3)),self.weights3.T)*sigmoid_derivative(self.layer2)),self.weights2.T)\n",
    "\n",
    "        \n",
    "        x=0\n",
    "        error_neuron=[]\n",
    "        while x<10:\n",
    "            list_temp=[]\n",
    "            for i in error_layer3:\n",
    "                list_temp.append(i[x])\n",
    "            error_neuron.append(np.asarray(list_temp))\n",
    "            x+=1\n",
    "            \n",
    "        error_neuron=np.asarray(error_neuron) \n",
    "        f=0\n",
    "        d_weights3=[]\n",
    "        d_weights2=[]\n",
    "        d_weights1=[]\n",
    "        while f<10:\n",
    "            d_weights3.append(np.dot(self.list2[f].T,(error_neuron[f].reshape(7988,1) * sigmoid_derivative(self.list3[f]))))\n",
    "            d_weights2.append(np.dot(self.list1[f].T,(np.dot((error_neuron[f].reshape(7988,1) * sigmoid_derivative(self.list3[f])),self.list_weights3[f].reshape(6,1).T)*sigmoid_derivative(self.list2[f]))))\n",
    "            d_weights1.append(np.dot(self.list_input[f].T,(np.dot((np.dot((error_neuron[f].reshape(7988,1) * sigmoid_derivative(self.list3[f])),self.list_weights3[f].reshape(6,1).T)*sigmoid_derivative(self.list2[f])),self.list_weights2[f].reshape(18,6).T)*sigmoid_derivative(self.list1[f]))))\n",
    "            f+=1\n",
    "        # update the weights with the derivative of the loss function\n",
    "\n",
    "        self.weights5 -= learning_rate*d_weights5\n",
    "        self.weights5 -= learning_rate*lambda_1*self.weights5\n",
    "\n",
    "        self.weights4 -= learning_rate*d_weights4\n",
    "        self.weights4 -= learning_rate*lambda_1*self.weights4\n",
    "        s=0\n",
    "        for d in d_weights3:\n",
    "            for b in d:\n",
    "\n",
    "                self.weights3[s][s//6]-=learning_rate*b\n",
    "                s+=1\n",
    "        \n",
    "        \n",
    "        \n",
    "        e=0\n",
    "        for i in d_weights2:\n",
    "            for j in i:\n",
    "                m=0\n",
    "                for k in j:\n",
    "                    self.weights2[e//6][(e//(18*6))*6+m]-=learning_rate*k\n",
    "                    e+=1\n",
    "                    m+=1\n",
    "        \n",
    "        e=0\n",
    "        for i in d_weights1:\n",
    "            for j in i:\n",
    "                m=0\n",
    "                for k in j:\n",
    "                    self.weights1[e//18][(e//(40*18))*18+m]-=learning_rate*k\n",
    "                    e+=1\n",
    "                    m+=1\n",
    "        \n",
    "        self.weights3 -=learning_rate*lambda_1*self.weights3\n",
    "        self.weights2 -=learning_rate*lambda_1*self.weights2\n",
    "        self.weights1 -=learning_rate*lambda_1*self.weights1\n",
    "        \n",
    "\n",
    "    def forecast (self, X_test, Y_test):\n",
    "        test_layer1 = sigmoid(np.dot(X_test, self.weights1))\n",
    "        test_layer2 = sigmoid(np.dot(test_layer1, self.weights2))\n",
    "        test_layer3 = sigmoid(np.dot(test_layer2, self.weights3))\n",
    "        test_layer4 = sigmoid(np.dot(test_layer3, self.weights4))\n",
    "        self.test_output = sigmoid(np.dot(test_layer4, self.weights5))\n",
    "        k=0\n",
    "        hg=0\n",
    "        bd=0\n",
    "        hd=0\n",
    "        bg=0\n",
    "        liste_matrix=[]\n",
    "        for i in self.test_output:\n",
    "            if (i>0.5 and Y_test[k]==1):\n",
    "                bd +=1\n",
    "            elif (i<0.5 and Y_test[k]==0):\n",
    "                hg +=1\n",
    "            elif(i<0.5 and Y_test[k]==1):\n",
    "                bg+=1\n",
    "            else:\n",
    "                hd +=1\n",
    "            k+=1\n",
    "        liste_matrix.append(hg)\n",
    "        liste_matrix.append(bg)\n",
    "        liste_matrix.append(hd)\n",
    "        liste_matrix.append(bd)\n",
    "        return liste_matrix\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "#     nn = NeuralNetwork(X_train,y_train, 0.33, 0.09)\n",
    "#     for i in range(100):\n",
    "#         nn.feedforward()\n",
    "#         nn.backprop()\n",
    "#     test=nn.forecast(X_test, y_test)\n",
    "#     print(nn.weights4)\n",
    "#     print(nn.test_output)\n",
    "        \n",
    "    maxi_out=0\n",
    "    liste_max=[]\n",
    "    liste_matrice=[]\n",
    "    list_w4=[]\n",
    "    for p in np.arange(0.22,0.8,0.03):\n",
    "\n",
    "        for j in np.arange(0,0.3,0.03):\n",
    "            nn = NeuralNetwork(X_train,y_train, p, j)\n",
    "            for i in range(60):\n",
    "                nn.feedforward()\n",
    "                nn.backprop()\n",
    "\n",
    "            test=nn.forecast(X_test, y_test)\n",
    "            if (test[0]+test[3])/(test[0]+test[1]+test[2]+test[3])>maxi_out:\n",
    "                maxi_out=(test[0]+test[3])/(test[0]+test[1]+test[2]+test[3])\n",
    "                liste_max.append((p,j))\n",
    "                liste_matrice.append(test)\n",
    "                list_w4.append(nn.weights4)\n",
    "                print(test)\n",
    "\n",
    "print(\"Accuracy of the best model is : \" + str(maxi_out))\n",
    "print(liste_max)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
