{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import esig.tosig as ts\n",
    "import sklearn\n",
    "import xgboost\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_data=pd.read_csv(r\"C:\\Users\\Colomban Basset\\Documents\\These Bitcoin\\4.Forecast\\bitcoin-historical-data V2\\coinbaseUSD_1-min_data_2014-12-01_to_2019-01-09.csv\")\n",
    "bit_data[\"date\"]=pd.to_datetime(bit_data[\"Timestamp\"],unit=\"s\").dt.date\n",
    "bit_data[\"hour\"]=pd.to_datetime(bit_data[\"Timestamp\"],unit=\"s\").dt.hour\n",
    "data=bit_data.pivot_table(index=['date','hour'],values='Close',aggfunc=np.mean)# group=bit_data.groupby(\"DateTime\")\n",
    "# data=group[\"Close\"].mean()\n",
    "# data=pd.DataFrame(data)\n",
    "data=data.sort_values(by=['date', 'hour'])\n",
    "# set 1 : 14 /03 : 1520985600\n",
    "# set 2 : 14/11 : 1542153600\n",
    "# set 3 : 25/05 : 1527206400\n",
    "# data.set_index('date', inplace=True)\n",
    "# initial_date = '2018-02-01'\n",
    "# finish_date = '2018-11_01'\n",
    "BTC_price_time = flattened.loc[len(data)-10000:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BTC_price_time['Log_return']=np.log(BTC_price_time['Close'])-np.log(BTC_price_time['Close'].shift(+1))\n",
    "BTC_price_time['Sign_return']=np.where(BTC_price_time.Log_return>=0,1,0)\n",
    "BTC_price_time['Next_return'] = BTC_price_time['Sign_return'].shift(-1)\n",
    "BTC_price_time['Previous_return'] = BTC_price_time['Sign_return'].shift(1)\n",
    "BTC_price_time['Change_sign'] = np.where(BTC_price_time.Sign_return!=BTC_price_time.Next_return,1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ljung Box test\n",
    "from  statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "a=statsmodels.stats.diagnostic.acorr_ljungbox(np.asarray(BTC_price_time['Log_return'][1:]), lags=15, boxpierce=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dickey Fuller test\n",
    "def test_stationarity(timeseries):\n",
    "\n",
    "    print ('<Results of Dickey-Fuller Test>')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4],\n",
    "                         index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print(dfoutput)\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "test_stationarity(BTC_price_time['Log_return'].fillna(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signature computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetWindow(x,h_window =30,f_window=10):\n",
    "\n",
    "    # First window\n",
    "    X = np.array(x.iloc[:h_window,]).reshape(1,-1)\n",
    "   \n",
    "    # Append next window\n",
    "    for i in range(1,len(x)-h_window+1):\n",
    "        x_i = np.array(x.iloc[i:i+h_window,]).reshape(1,-1)\n",
    "        X = np.append(X,x_i, axis=0)\n",
    "        \n",
    "    # Cut the end that we can't use to predict future price\n",
    "    rolling_window = (pd.DataFrame(X)).iloc[:-f_window,]\n",
    "    return rolling_window\n",
    "\n",
    "#input = panda, historical window, future window\n",
    "def GetNextMean(x,h_window=30,f_window=10):\n",
    "    return pd.DataFrame((x.rolling(f_window).mean().iloc[h_window+f_window-1:,]))\n",
    "\n",
    "#Function add time to the data set\n",
    "def AddTime(X):\n",
    "    t = np.linspace(0,1,len(X))\n",
    "    return np.c_[t, X]\n",
    "\n",
    "\n",
    "#Function for Lead lag transform\n",
    "def Lead(X):\n",
    "    \n",
    "    s = X.shape\n",
    "    x_0 = X[:,0]\n",
    "    Lead = np.delete(np.repeat(x_0,2),0).reshape(-1,1)\n",
    "     \n",
    "    for j in range(1,s[1]):\n",
    "        x_j = X[:,j]\n",
    "        x_j_lead = np.delete(np.repeat(x_j,2),0).reshape(-1,1)\n",
    "        Lead = np.concatenate((Lead,x_j_lead), axis =1)\n",
    "     \n",
    "    return Lead\n",
    "\n",
    "#Function for Lead lag transform\n",
    "def Lag(X):\n",
    "    \n",
    "    s = X.shape\n",
    "    x_0 = X[:,0]\n",
    "    Lag = np.delete(np.repeat(x_0,2),-1).reshape(-1,1)\n",
    "  \n",
    "    for j in range(1,s[1]):\n",
    "        x_j = X[:,j]\n",
    "        x_j_lag  = np.delete(np.repeat(x_j,2),-1).reshape(-1,1)\n",
    "        Lag = np.concatenate((Lag,x_j_lag), axis = 1)\n",
    "        \n",
    "    return Lag\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We use only open price\n",
    "close_price = BTC_price_time.loc[:,'Log_return']\n",
    "h_window = 3\n",
    "f_window = 1\n",
    "sig_level = 3\n",
    "\n",
    "# mean next price\n",
    "y = GetNextMean(close_price, h_window = h_window , f_window = f_window)\n",
    "\n",
    "#GetNextMean(close_price, h_window = h_window , f_window = f_window)\n",
    "\n",
    "# normal window features\n",
    "X_window = AddTime(GetWindow(close_price, h_window = h_window, f_window = f_window))\n",
    "X_window = pd.DataFrame(X_window)\n",
    "\n",
    "\n",
    "# signature features\n",
    "#Consider only area that has at least f_window future prices left\n",
    "close_price_slice = close_price.iloc[0:(len(close_price)-(f_window))]\n",
    "close_price_array = np.array(close_price_slice).reshape(-1,1)\n",
    "lag = Lag(close_price_array)\n",
    "lead = Lead(AddTime(close_price_array))\n",
    "#concatenate everything to get a datastream\n",
    "stream = np.concatenate((lead,lag), axis = 1)\n",
    "X_sig = [ts.stream2sig(stream[0:2*h_window-1], sig_level)]\n",
    "print(stream[11])\n",
    "for i in range(1,(len(close_price)-(f_window)-(h_window)+1)):\n",
    "    stream_i = stream[2*i: 2*(i+h_window)-1]\n",
    "    signature_i = [ts.stream2sig(stream_i, sig_level)]\n",
    "    X_sig = np.append(X_sig, signature_i, axis=0)\n",
    "\n",
    "X_sig = pd.DataFrame(X_sig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nomalisation of the signature terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition as decomp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler=StandardScaler()#instantiate\n",
    "scaler.fit(X_sig) # compute the mean and standard which will be used in the next command\n",
    "X_scaled=scaler.transform(X_sig)\n",
    "\n",
    "y=BTC_price_time.loc[:,'Sign_return']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paradigm 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_len = 600\n",
    "train_len = len(y) - test_len\n",
    "# y_0=y.iloc[h_window:,]\n",
    "# i=1\n",
    "#     list_full=\n",
    "# while i<10:\n",
    "#     array=[]\n",
    "#     array.append(0)\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X,y into train and test region\n",
    "\n",
    "\n",
    "X_train = X_scaled[:train_len-h_window,]\n",
    "y_train = y.iloc[h_window:train_len,]\n",
    "X_test = X_scaled[train_len-h_window:,]\n",
    "y_test = y.iloc[train_len:len(y),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train)\n",
    "y_train = pd.DataFrame(y_train)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "y_test = pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.fillna(X_train.mean())\n",
    "y_train= y_train.fillna(y_train.mean())\n",
    "X_test = X_test.fillna(X_train.mean())\n",
    "y_test= y_test.fillna(y_train.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paradigm 1 : Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.linear_model.SGDClassifier(loss='log', penalty='elasticnet', alpha=0.0001, l1_ratio=0.15)\n",
    "param_search = {'alpha': [1e-5,1e-4,1e-3, 1e-2,1e-1,2e-1, 5e-1, 1], 'l1_ratio':[0.15,0.3,0.45,0.6,0.75,0.9]}\n",
    "myModel = GridSearchCV(estimator=SGDClassifier(alpha = 0.1,\n",
    "                                       \n",
    "                                    \n",
    "                                       max_iter = 10e5,\n",
    "                                       penalty='elasticnet',\n",
    "                                       loss='log'),\n",
    "                        param_grid = param_search,\n",
    "                        cv = TimeSeriesSplit(n_splits=8),\n",
    "                        n_jobs=-1)\n",
    "\n",
    "myModel.fit(X_train, y_train)\n",
    "\n",
    "#Make predictions\n",
    "y_train_predict = myModel.predict(X_train)\n",
    "y_test_predict = myModel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_test_predict)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paradigm 1 : Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='poly', gamma=0.2, C=6)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paradigm 1 : Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#mean absolute percentage error\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "#Perform a GridsearchCV\n",
    "param_grid = [\n",
    "    {'classifier' : [LogisticRegression()],\n",
    "     'classifier__penalty' : [ 'l2'],\n",
    "    'classifier__C' : np.logspace(-20, -4, 4, 20, 50),\n",
    "    'classifier__solver' : ['newton-cg']},\n",
    "    {'classifier' : [RandomForestClassifier()],\n",
    "    'classifier__n_estimators' : list(range(10,150,160)),\n",
    "    'classifier__max_features' : list(range(5,20, 40))}\n",
    "]\n",
    "pipe = Pipeline([('classifier' , RandomForestClassifier())])\n",
    "myModel = GridSearchCV(pipe, param_grid = param_grid, cv = 8, verbose=True, n_jobs=-3)\n",
    "\n",
    "# myModel.fit(X_scaled[:len(X_scaled)-240-1], values[h_window+1:aa-240])\n",
    "myModel.fit(X_train, y_train)\n",
    "#Make predictions\n",
    "# y_train_predict = myModel.predict(X_scaled)\n",
    "y_test_predict = myModel.predict(X_test)\n",
    "\n",
    "#Calculate error\n",
    "# error_train = mean_absolute_error(values[h_window:aa], y_train_predict)        \n",
    "# error_test = mean_absolute_error(y_test, y_test_predict)\n",
    "# p_error_train = mean_absolute_percentage_error(np.array(y_train).reshape(-1,1), np.array(y_train_predict).reshape(-1,1))\n",
    "# p_error_test = mean_absolute_percentage_error(np.array(y_test).reshape(-1,1), np.array(y_test_predict).reshape(-1,1))\n",
    "\n",
    "# print('mae_train:{0:.3f} = {1:.3f}%\\n'.format(error_train, p_error_train))\n",
    "# print('mae_test:{0:.3f} = {1:.3f}%'.format(error_test, p_error_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_test_predict)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paradigm 1 : XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics   #Additional scklearn functions\n",
    "from sklearn.model_selection import GridSearchCV  #Perforing grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#GridsearchCV    \n",
    "param_search = {\n",
    "    'max_depth':[4,5,6]\n",
    "    ,'min_child_weight':[4,5,6]\n",
    "    ,'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "\n",
    "myModel = GridSearchCV(estimator=XGBRegressor(\n",
    "                        objective='binary:logistic',\n",
    "                        learning_rate=0.01,\n",
    "                        n_estimators=500,\n",
    "                        max_depth=5,\n",
    "                        min_child_weight=5,\n",
    "                        gamma=0,\n",
    "                        subsample=0.8,\n",
    "                        colsample_bytree=0.8, \n",
    "                        eval_metric ='mae',\n",
    "                        reg_alpha=0.05\n",
    "                        ),\n",
    "                       param_grid = param_search,\n",
    "                       cv = TimeSeriesSplit(n_splits=4),n_jobs=-1\n",
    "                      )\n",
    "\n",
    "#Fit model\n",
    "myModel.fit(X_train, y_train)\n",
    "\n",
    "#Make predictions\n",
    "# y_train_predict = myModel.predict(X_train)\n",
    "y_test_predict = myModel.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positif=0\n",
    "negatif=0\n",
    "extreme=0\n",
    "inter=0\n",
    "i=0\n",
    "hg,hd,bg,bd=0\n",
    "while i < len(y_test_predict):\n",
    "    if y_test_predict[i]>0.5:\n",
    "        if np.asscalar(y_test.iloc[i])>0.5:\n",
    "            hg+=1\n",
    "            extreme+=1\n",
    "        else:\n",
    "            hd+=1\n",
    "            extreme+=1\n",
    "    elif y_test_predict[i]<0.5:\n",
    "        if np.asscalar(y_test.iloc[i])<0.5:\n",
    "            bd+=1\n",
    "        else:\n",
    "            bg+=1\n",
    "    else:\n",
    "        inter+=1\n",
    "    i+=1\n",
    "            \n",
    "print('[',hg,hd)\n",
    "print(' ', bg,bd,']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(myModel.best_estimator_.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paradigm 1 : Keras Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# X = pd.DataFrame(X_scaled)\n",
    "# y = pd.DataFrame(values[h_window+1:len(values)])\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "#First Hidden Layer\n",
    "classifier.add(Dense(10, activation='relu', kernel_initializer='random_normal', input_dim=40))\n",
    "#Second  Hidden Layer\n",
    "# classifier.add(Dense(10, activation='relu', kernel_initializer='random_normal'))\n",
    "# classifier.add(Dense(5, activation='relu', kernel_initializer='random_normal'))\n",
    "\n",
    "#Output Layer\n",
    "classifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the neural network\n",
    "classifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the data to the training dataset\n",
    "classifier.fit(X_train,y_train, batch_size=10, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=classifier.predict(X_test)\n",
    "y_pred =(y_pred>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paradigm 2 : Forecast of individual signature terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from pmdarima import auto_arima\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forecast signature terms with ARIMA\n",
    "o=0\n",
    "list_col=[]\n",
    "X_scaled=pd.DataFrame(X_scaled)\n",
    "X_scaled=X_scaled.fillna(X_scaled.mean())\n",
    "while o<40:\n",
    "    series=X_scaled.iloc[len(X_scaled)-1000:,o]\n",
    "    print(len(series))\n",
    "    p=0\n",
    "    list_rows=[]\n",
    "    param=200\n",
    "    while p<param:\n",
    "        train = series[:len(series)-param+p]\n",
    "        test = series[len(series)-param+p:len(series)-param+p+1]\n",
    "        if p==0:\n",
    "            print('test',test)\n",
    "        model= auto_arima(series, start_p=0, start_q=0,\n",
    "                               max_p=3, max_q=3, \n",
    "                               seasonal=False,\n",
    "                               d=1, trace=False,\n",
    "                               error_action='ignore',  \n",
    "                               suppress_warnings=True, \n",
    "                               stepwise=True)\n",
    "\n",
    "        \n",
    "        model.fit(train)\n",
    "        future_forecast = model.predict(n_periods=len(test))\n",
    "        list_rows.append(future_forecast)\n",
    "        \n",
    "        p+=1\n",
    "    list_col.append(list_rows)\n",
    "    print(list_col)\n",
    "    \n",
    "    o+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paradigm 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent neural networks- LSTMs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "y_train = np.reshape(y_train, (y_train.shape[0], 1, y_train.shape[1]))\n",
    "\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "y_test = np.reshape(y_test, (y_test.shape[0], 1, y_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# compile and fit the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "history = model.fit(X_train,y_train, epochs=200, batch_size=7988, validation_data=(X_test, y_test), callbacks=[EarlyStopping(monitor='val_loss', patience=10)], verbose=1, shuffle=False)\n",
    "y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=0\n",
    "hg=0\n",
    "hd=0\n",
    "bg=0\n",
    "bd=0\n",
    "for i in y_pred:\n",
    "    if (i >0.5 and y_test[j]==1):\n",
    "        bd +=1\n",
    "    elif (i <0.5 and y_test[j]==0):\n",
    "        hg+=1\n",
    "    elif(i<0.5 and y_test[j]==1):\n",
    "        bg+=1\n",
    "    else:\n",
    "        hd+=1\n",
    "    j+=1\n",
    "print(hg,bg,hd,bd)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Own deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vector=y.iloc[h_window:len(y)]\n",
    "y_train=y_vector[9:]\n",
    "list_x_train=[]\n",
    "k=9\n",
    "while k<len(X_scaled):\n",
    "    list_temp=[]\n",
    "    p=0\n",
    "    while p<=9:\n",
    "        list_temp.append(X_scaled[k-p])\n",
    "        p+=1\n",
    "    list_x_train.append(list_temp)\n",
    "    k+=1\n",
    "\n",
    "list_fin_train=[]\n",
    "for i in list_x_train:\n",
    "    list_temp=[]\n",
    "    for j in i:\n",
    "        for k in j:\n",
    "            list_temp.append(k)\n",
    "    list_fin_train.append(list_temp)\n",
    "\n",
    "X_full=np.asarray(list_fin_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_full=np.asarray(y_train).reshape((len(y_train), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_full[1:len(X_full)-1999]\n",
    "X_test=X_full[len(X_full)-1999:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_full[1:len(y_full)-1999]\n",
    "y_test=y_full[len(y_full)-1999:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1+ np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1.0 - x)\n",
    "def ReLU(x): \n",
    "    j=0\n",
    "    for i in x:\n",
    "        p=0\n",
    "        for k in i:\n",
    "            if k <0:\n",
    "                x[j][p]=0\n",
    "            p+=1\n",
    "        j+=1\n",
    "    return x\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y, lamb1, learning_r):\n",
    "        self.input = x\n",
    "        list_weights1 = []\n",
    "        j=0\n",
    "        while j<self.input.shape[1]:\n",
    "            list_temp=[]\n",
    "            k=j//40\n",
    "            m=0\n",
    "            list_temp+=[0]*180\n",
    "            r=18*(k+1)\n",
    "            b=r-18\n",
    "            for i in range(b,r):\n",
    "                list_temp[i]=random.uniform(-150,150)\n",
    "            list_weights1.append(np.asarray(list_temp))\n",
    "            j+=1\n",
    "            \n",
    "        self.weights1   = np.asarray(list_weights1)\n",
    "        \n",
    "        list_weights2=[]\n",
    "        j=0\n",
    "        while j<180:\n",
    "            list_temp=[]\n",
    "            k=j//18\n",
    "            m=0\n",
    "            list_temp+=[0]*60\n",
    "            r=6*(k+1)\n",
    "            b=r-6\n",
    "            for i in range(b,r):\n",
    "                list_temp[i]=random.uniform(-150,150)\n",
    "            list_weights2.append(np.asarray(list_temp))\n",
    "            j+=1\n",
    "        \n",
    "        self.weights2   = np.asarray(list_weights2)\n",
    "        \n",
    "        list_weights3=[]\n",
    "        j=0\n",
    "        while j<60:\n",
    "            list_temp=[]\n",
    "            k=j//6\n",
    "            m=0\n",
    "            list_temp+=[0]*10\n",
    "            r=1*(k+1)\n",
    "            b=r-1\n",
    "            for i in range(b,r):\n",
    "                list_temp[i]=random.uniform(-150,150)\n",
    "            list_weights3.append(np.asarray(list_temp))\n",
    "            j+=1\n",
    "        self.weights3   = np.asarray(list_weights3)\n",
    "        self.weights4   = np.random.uniform(low=-150,high=150, size=(10,5))\n",
    "        self.weights5   = np.random.uniform(low=-150,high=150, size=(5,1))\n",
    "        self.y          = y\n",
    "        self.output     = np.zeros(self.y.shape)\n",
    "        \n",
    "        self.lamb1=lamb1\n",
    "        self.learning_r=learning_r\n",
    "        \n",
    "    def feedforward(self):\n",
    "        #ON peut tenter avec ReLU aussi \n",
    "#         self.layer1=ReLU(np.dot(self.input, self.weights1))\n",
    "#         self.layer2=ReLU(np.dot(self.layer1, self.weights2))\n",
    "#         self.layer3=ReLU(np.dot(self.layer2, self.weights3))\n",
    "#         self.layer4=ReLU(np.dot(self.layer3, self.weights4))\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.layer2 = sigmoid(np.dot(self.layer1, self.weights2))\n",
    "        self.layer3 = sigmoid(np.dot(self.layer2, self.weights3))\n",
    "        self.layer4 = sigmoid(np.dot(self.layer3, self.weights4))\n",
    "        self.output = sigmoid(np.dot(self.layer4, self.weights5))\n",
    "        \n",
    "    def backprop(self):\n",
    "        # application of the chain rule to find derivative of the loss function with respect to weights\n",
    "        self.list3=[]\n",
    "        self.list2=[]\n",
    "        self.list1=[]\n",
    "        self.list_input=[]\n",
    "        self.list_weights3=[]\n",
    "        self.list_weights2=[]\n",
    "        p=0\n",
    "        while p<10:\n",
    "            list3=[]\n",
    "            \n",
    "            for i in self.layer3:\n",
    "                list3.append(i[p])\n",
    "            list2=[]\n",
    "            for i in self.layer2:\n",
    "                list2.append(i[p*6:(p+1)*6])\n",
    "            list1=[]\n",
    "            for i in self.layer1:\n",
    "                list1.append(i[p*18:(p+1)*18])\n",
    "            list_input=[]\n",
    "            for i in self.input:\n",
    "                list_input.append(i[p*40:(p+1)*40])\n",
    "            list_weights3=[]\n",
    "            \n",
    "            f=0\n",
    "            for i in self.weights3:\n",
    "                if f>=(p)*6 and f<(p+1)*6:\n",
    "                    list_weights3.append(i[p])\n",
    "                f+=1\n",
    "\n",
    "            list_weights2=[]\n",
    "\n",
    "            for i in self.weights2[p*18:(p+1)*18]:\n",
    "                list_weights2.append(i[p*6:(p+1)*6])\n",
    "\n",
    "            \n",
    "            self.list_input.append(np.asarray(list_input))\n",
    "            self.list1.append(np.asarray(list1))\n",
    "            self.list2.append(np.asarray(list2))\n",
    "            self.list3.append(np.asarray(list3))\n",
    "            self.list_weights3.append(np.asarray(list_weights3))\n",
    "            self.list_weights2.append(np.asarray(list_weights2))\n",
    "            \n",
    "            p+=1\n",
    "        \n",
    "        self.list_input=np.asarray(self.list_input)     \n",
    "        self.list1=np.asarray(self.list1)\n",
    "        self.list2=np.asarray(self.list2)\n",
    "        self.list3=np.asarray(self.list3)\n",
    "        self.list3=self.list3.reshape(10,7988,1)\n",
    "        self.list_weights3=np.asarray(self.list_weights3)\n",
    "        self.list_weights2=np.asarray(self.list_weights2)\n",
    "        \n",
    "        lambda_1=self.lamb1\n",
    "        lambda_2=0.5\n",
    "        learning_rate=self.learning_r\n",
    "        error_output=(self.y*np.log(self.output)+(1-self.y)*np.log(1-self.output))\n",
    "        d_error_output=self.y/self.output +(1-self.y)/(self.output-1)\n",
    "\n",
    "        d_weights5 = np.dot(self.layer4.T, (-2*(self.y-self.output) * sigmoid_derivative(self.output)))\n",
    "        d_weights4 = np.dot(self.layer3.T, (np.dot((-2*(self.y-self.output) * sigmoid_derivative(self.output)), self.weights5.T) * sigmoid_derivative(self.layer4)))\n",
    "        error_layer3=(np.dot((np.dot(-2*(self.y-self.output) * sigmoid_derivative(self.output), self.weights5.T) * sigmoid_derivative(self.layer4)),self.weights4.T))\n",
    "        error_layer2=np.dot((np.dot((np.dot(-2*(self.y-self.output) * sigmoid_derivative(self.output), self.weights5.T) * sigmoid_derivative(self.layer4)),self.weights4.T) * sigmoid_derivative(self.layer3)),self.weights3.T)\n",
    "        error_layer1=np.dot((np.dot((np.dot((np.dot(-2*(self.y-self.output) * sigmoid_derivative(self.output), self.weights5.T) * sigmoid_derivative(self.layer4)),self.weights4.T) * sigmoid_derivative(self.layer3)),self.weights3.T)*sigmoid_derivative(self.layer2)),self.weights2.T)\n",
    "\n",
    "        \n",
    "        x=0\n",
    "        error_neuron=[]\n",
    "        while x<10:\n",
    "            list_temp=[]\n",
    "            for i in error_layer3:\n",
    "                list_temp.append(i[x])\n",
    "            error_neuron.append(np.asarray(list_temp))\n",
    "            x+=1\n",
    "            \n",
    "        error_neuron=np.asarray(error_neuron) \n",
    "        f=0\n",
    "        d_weights3=[]\n",
    "        d_weights2=[]\n",
    "        d_weights1=[]\n",
    "        while f<10:\n",
    "            d_weights3.append(np.dot(self.list2[f].T,(error_neuron[f].reshape(7988,1) * sigmoid_derivative(self.list3[f]))))\n",
    "            d_weights2.append(np.dot(self.list1[f].T,(np.dot((error_neuron[f].reshape(7988,1) * sigmoid_derivative(self.list3[f])),self.list_weights3[f].reshape(6,1).T)*sigmoid_derivative(self.list2[f]))))\n",
    "            d_weights1.append(np.dot(self.list_input[f].T,(np.dot((np.dot((error_neuron[f].reshape(7988,1) * sigmoid_derivative(self.list3[f])),self.list_weights3[f].reshape(6,1).T)*sigmoid_derivative(self.list2[f])),self.list_weights2[f].reshape(18,6).T)*sigmoid_derivative(self.list1[f]))))\n",
    "            f+=1\n",
    "        # update the weights with the derivative of the loss function\n",
    "\n",
    "        self.weights5 -= learning_rate*d_weights5\n",
    "        self.weights5 -= learning_rate*lambda_1*self.weights5\n",
    "\n",
    "        self.weights4 -= learning_rate*d_weights4\n",
    "        self.weights4 -= learning_rate*lambda_1*self.weights4\n",
    "        s=0\n",
    "        for d in d_weights3:\n",
    "            for b in d:\n",
    "\n",
    "                self.weights3[s][s//6]-=learning_rate*b\n",
    "                s+=1\n",
    "        \n",
    "        \n",
    "        \n",
    "        e=0\n",
    "        for i in d_weights2:\n",
    "            for j in i:\n",
    "                m=0\n",
    "                for k in j:\n",
    "                    self.weights2[e//6][(e//(18*6))*6+m]-=learning_rate*k\n",
    "                    e+=1\n",
    "                    m+=1\n",
    "        \n",
    "        e=0\n",
    "        for i in d_weights1:\n",
    "            for j in i:\n",
    "                m=0\n",
    "                for k in j:\n",
    "                    self.weights1[e//18][(e//(40*18))*18+m]-=learning_rate*k\n",
    "                    e+=1\n",
    "                    m+=1\n",
    "        \n",
    "        self.weights3 -=learning_rate*lambda_1*self.weights3\n",
    "        self.weights2 -=learning_rate*lambda_1*self.weights2\n",
    "        self.weights1 -=learning_rate*lambda_1*self.weights1\n",
    "        \n",
    "\n",
    "    def forecast (self, X_test, Y_test):\n",
    "        test_layer1 = sigmoid(np.dot(X_test, self.weights1))\n",
    "        test_layer2 = sigmoid(np.dot(test_layer1, self.weights2))\n",
    "        test_layer3 = sigmoid(np.dot(test_layer2, self.weights3))\n",
    "        test_layer4 = sigmoid(np.dot(test_layer3, self.weights4))\n",
    "        self.test_output = sigmoid(np.dot(test_layer4, self.weights5))\n",
    "        k=0\n",
    "        hg=0\n",
    "        bd=0\n",
    "        hd=0\n",
    "        bg=0\n",
    "        liste_matrix=[]\n",
    "        for i in self.test_output:\n",
    "            if (i>0.5 and Y_test[k]==1):\n",
    "                bd +=1\n",
    "            elif (i<0.5 and Y_test[k]==0):\n",
    "                hg +=1\n",
    "            elif(i<0.5 and Y_test[k]==1):\n",
    "                bg+=1\n",
    "            else:\n",
    "                hd +=1\n",
    "            k+=1\n",
    "        liste_matrix.append(hg)\n",
    "        liste_matrix.append(bg)\n",
    "        liste_matrix.append(hd)\n",
    "        liste_matrix.append(bd)\n",
    "        return liste_matrix\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "#     nn = NeuralNetwork(X_train,y_train, 0.33, 0.09)\n",
    "#     for i in range(100):\n",
    "#         nn.feedforward()\n",
    "#         nn.backprop()\n",
    "#     test=nn.forecast(X_test, y_test)\n",
    "#     print(test)\n",
    "#     print(nn.test_output)\n",
    "        \n",
    "    maxi_out=0\n",
    "    liste_max=[]\n",
    "    liste_matrice=[]\n",
    "    for p in np.arange(0.1,0.9,0.03):\n",
    "\n",
    "        for j in np.arange(0,1,0.03):\n",
    "            nn = NeuralNetwork(X_train,y_train, p, j)\n",
    "            for i in range(60):\n",
    "                nn.feedforward()\n",
    "                nn.backprop()\n",
    "\n",
    "            test=nn.forecast(X_test, y_test)\n",
    "            if (test[0]+test[3])/(test[0]+test[1]+test[2]+test[3])>maxi_out:\n",
    "                maxi_out=(test[0]+test[3])/(test[0]+test[1]+test[2]+test[3])\n",
    "                liste_max.append((p,j))\n",
    "                liste_matrice.append(test)\n",
    "                print(test)\n",
    "\n",
    "print(maxi_out)\n",
    "print(liste_max)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
